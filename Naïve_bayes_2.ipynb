{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?"
      ],
      "metadata": {
        "id": "r6mSuXY1b3-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find the probability that an employee is a smoker given that he/she uses the company's health insurance plan, we can use Bayes' theorem and the information provided in the problem.\n",
        "\n",
        "### Given Data\n",
        "- Let \\( A \\) be the event that an employee uses the health insurance plan.\n",
        "- Let \\( B \\) be the event that an employee is a smoker.\n",
        "\n",
        "From the problem:\n",
        "- \\( P(A) = 0.7 \\) (70% of employees use the health insurance plan)\n",
        "- \\( P(B|A) = 0.4 \\) (40% of employees who use the plan are smokers)\n",
        "\n",
        "We want to find \\( P(B|A) \\), which is already given as 0.4.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "The probability that an employee is a smoker given that he/she uses the health insurance plan is:\n",
        "\n",
        "\\[\n",
        "P(B|A) = 0.4 \\text{ or } 40\\%\n",
        "\\]\n",
        "\n",
        "Therefore, the answer is **40%**."
      ],
      "metadata": {
        "id": "XKz6_CPZb9-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
      ],
      "metadata": {
        "id": "dn__obWLcAnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bernoulli Naive Bayes and Multinomial Naive Bayes are both variations of the Naive Bayes classifier, but they are suited for different types of data and have distinct assumptions about the feature distributions. Here’s a breakdown of the key differences:\n",
        "\n",
        "### 1. **Feature Representation**\n",
        "   - **Bernoulli Naive Bayes**:\n",
        "     - Assumes binary features (0 or 1), where each feature represents the presence or absence of an attribute.\n",
        "     - Often used for text classification tasks where features indicate whether a word appears in a document.\n",
        "   - **Multinomial Naive Bayes**:\n",
        "     - Assumes that features represent counts or frequencies of occurrences.\n",
        "     - Suitable for text classification where features are the counts of words or tokens, allowing for multiple occurrences.\n",
        "\n",
        "### 2. **Input Data Requirements**\n",
        "   - **Bernoulli Naive Bayes**:\n",
        "     - Requires binary data (features must be either 0 or 1).\n",
        "     - For example, in a document classification task, if the word \"apple\" appears, it is represented as 1; otherwise, it is 0.\n",
        "   - **Multinomial Naive Bayes**:\n",
        "     - Can handle integer-valued features that represent counts.\n",
        "     - For example, the word \"apple\" could appear 3 times in a document, which would be represented as a count of 3.\n",
        "\n",
        "### 3. **Mathematical Formulation**\n",
        "   - **Bernoulli Naive Bayes**:\n",
        "     - The likelihood of a document given a class is calculated using the presence (1) or absence (0) of features:\n",
        "       \\[\n",
        "       P(X|C) = \\prod_{i=1}^{n} P(X_i | C)^{x_i}\n",
        "       \\]\n",
        "       where \\(x_i\\) is 1 if feature \\(i\\) is present and 0 if absent.\n",
        "   - **Multinomial Naive Bayes**:\n",
        "     - The likelihood is calculated based on the counts of features:\n",
        "       \\[\n",
        "       P(X|C) = \\frac{(n!)!}{(\\sum_{i=1}^{n} x_i)!} \\prod_{i=1}^{n} P(X_i | C)^{x_i}\n",
        "       \\]\n",
        "       where \\(x_i\\) is the count of feature \\(i\\).\n",
        "\n",
        "### 4. **Application Context**\n",
        "   - **Bernoulli Naive Bayes**:\n",
        "     - Commonly used in situations where the presence or absence of features is important, like spam detection (words are either present or absent).\n",
        "   - **Multinomial Naive Bayes**:\n",
        "     - Often used in text classification tasks like document classification or sentiment analysis, where the frequency of words is more informative than their mere presence.\n",
        "\n",
        "### Summary of Differences\n",
        "\n",
        "| Feature                     | Bernoulli Naive Bayes       | Multinomial Naive Bayes      |\n",
        "|-----------------------------|-----------------------------|-------------------------------|\n",
        "| Feature Type                | Binary (0/1)                | Counts (integers)            |\n",
        "| Data Input                  | Presence/absence of features | Frequencies of features       |\n",
        "| Likelihood Calculation       | Based on presence/absence    | Based on counts              |\n",
        "| Common Use Cases            | Spam detection, binary classification | Text classification, sentiment analysis |\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "In summary, the choice between Bernoulli Naive Bayes and Multinomial Naive Bayes primarily depends on the nature of the input data. If the features are binary, use Bernoulli; if they represent counts or frequencies, use Multinomial."
      ],
      "metadata": {
        "id": "B5CIMxzbcIFT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. How does Bernoulli Naive Bayes handle missing values?"
      ],
      "metadata": {
        "id": "w1c6PDjncKyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bernoulli Naive Bayes, like other Naive Bayes classifiers, has a straightforward approach to handling missing values, primarily because it assumes independence between features and is based on probabilities. Here’s how it handles missing values:\n",
        "\n",
        "### 1. **Ignoring Missing Features**\n",
        "When a feature is missing for a given instance, Bernoulli Naive Bayes typically ignores that feature during the computation of probabilities. This means that if a feature is not present (i.e., the feature is missing), it does not contribute to the probability calculation for that instance. This behavior is largely due to the independence assumption inherent in Naive Bayes algorithms.\n",
        "\n",
        "### 2. **Using Conditional Probabilities**\n",
        "The algorithm will still calculate the probabilities of the remaining features that are present. For instance, if you have a feature vector with three features and one feature is missing, the algorithm will use the probabilities associated with the two remaining features to compute the class probabilities.\n",
        "\n",
        "### 3. **Effect on Class Probability Calculation**\n",
        "When calculating the class probabilities, missing values can affect the total count of occurrences for the available features. If a feature is missing, the associated conditional probability for that feature in a specific class will not be considered in the product of probabilities for that instance. This might result in slightly skewed probability estimates, but it is a reasonable approach given the assumptions of the model.\n",
        "\n",
        "### 4. **Imputation (Optional)**\n",
        "If the proportion of missing values is substantial or if you want to mitigate the impact of missing values on the classification performance, you can consider imputing missing values before applying the Bernoulli Naive Bayes classifier. Common imputation techniques include:\n",
        "   - Replacing missing values with the mode of the feature (for categorical features).\n",
        "   - Using statistical methods to estimate missing values based on other available data.\n",
        "\n",
        "### Example Scenario\n",
        "Assume you have a feature set as follows:\n",
        "\n",
        "- Features: \\(X_1\\), \\(X_2\\), \\(X_3\\)\n",
        "- Missing Value: \\(X_2\\)\n",
        "\n",
        "When predicting the class for an instance with a missing value for \\(X_2\\), Bernoulli Naive Bayes would compute the class probability \\(P(Class | X_1, X_3)\\), completely ignoring \\(X_2\\). The likelihood is calculated as:\n",
        "\\[\n",
        "P(Class | X_1, X_3) = P(Class) \\cdot P(X_1 | Class) \\cdot P(X_3 | Class)\n",
        "\\]\n",
        "\n",
        "### Conclusion\n",
        "In summary, Bernoulli Naive Bayes handles missing values by ignoring them during the probability calculations for the remaining features. This allows the classifier to still make predictions based on the available data, although it may not be the most robust approach if missing values are prevalent. Therefore, careful consideration of data preprocessing, including imputation strategies, can enhance model performance in the presence of missing data."
      ],
      "metadata": {
        "id": "z7xxE6upcLxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
      ],
      "metadata": {
        "id": "2LPMuAdJcN2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, Gaussian Naive Bayes can be used for multi-class classification. In fact, Naive Bayes classifiers, including Gaussian Naive Bayes, are inherently designed to handle multiple classes without any additional modification. Here's how it works:\n",
        "\n",
        "### How Gaussian Naive Bayes Handles Multi-Class Classification\n",
        "\n",
        "1. **Multiple Class Labels**:\n",
        "   - In a multi-class classification problem, the target variable can take on three or more distinct classes (e.g., Class 1, Class 2, Class 3).\n",
        "   - Gaussian Naive Bayes calculates the probabilities for each class based on the features provided.\n",
        "\n",
        "2. **Probability Calculation**:\n",
        "   - For each class \\(C_k\\), Gaussian Naive Bayes calculates the likelihood of the features given that class using the Gaussian (normal) distribution.\n",
        "   - The probability of each feature \\(X_i\\) given the class \\(C_k\\) is computed using the formula for the Gaussian distribution:\n",
        "     \\[\n",
        "     P(X_i | C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(X_i - \\mu)^2}{2\\sigma^2}\\right)\n",
        "     \\]\n",
        "     where \\(\\mu\\) is the mean and \\(\\sigma^2\\) is the variance of the feature \\(X_i\\) for the class \\(C_k\\).\n",
        "\n",
        "3. **Class Prior Probabilities**:\n",
        "   - The prior probability for each class \\(P(C_k)\\) is estimated from the training data by calculating the proportion of instances belonging to each class.\n",
        "\n",
        "4. **Bayes' Theorem**:\n",
        "   - Using Bayes' theorem, the posterior probability for each class given the features is computed:\n",
        "     \\[\n",
        "     P(C_k | X) = \\frac{P(C_k) \\cdot P(X | C_k)}{P(X)}\n",
        "     \\]\n",
        "   - The denominator \\(P(X)\\) is the same for all classes and can be ignored when comparing probabilities, allowing for the simplification:\n",
        "     \\[\n",
        "     P(C_k | X) \\propto P(C_k) \\cdot P(X | C_k)\n",
        "     \\]\n",
        "\n",
        "5. **Choosing the Class**:\n",
        "   - Finally, the predicted class for a new instance is the one with the highest posterior probability:\n",
        "     \\[\n",
        "     \\text{Predicted Class} = \\arg\\max_{C_k} P(C_k | X)\n",
        "     \\]\n",
        "\n",
        "### Advantages of Using Gaussian Naive Bayes for Multi-Class Classification\n",
        "\n",
        "- **Simplicity**: Gaussian Naive Bayes is simple to implement and computationally efficient.\n",
        "- **Assumption of Independence**: It assumes that features are independent given the class, which simplifies calculations, even in multi-class settings.\n",
        "- **Handles Continuous Data**: It can handle continuous features well, as it uses the Gaussian distribution for likelihood estimation.\n",
        "\n",
        "### Example Use Case\n",
        "Gaussian Naive Bayes is often used in multi-class classification problems, such as:\n",
        "\n",
        "- Text classification (e.g., categorizing documents into different topics).\n",
        "- Handwriting recognition (classifying characters).\n",
        "- Medical diagnosis (predicting diseases based on patient symptoms).\n",
        "\n",
        "### Conclusion\n",
        "In summary, Gaussian Naive Bayes is well-suited for multi-class classification tasks. It effectively computes class probabilities and makes predictions based on the assumptions of the Naive Bayes framework, allowing it to classify instances into multiple categories seamlessly."
      ],
      "metadata": {
        "id": "C7eMsvLScSzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. Assignment:\n",
        "#Data preparation:\n",
        "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
        "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
        "is spam or not based on several input features.\n",
        "\n",
        "#Implementation:\n",
        "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
        "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
        "dataset. You should use the default hyperparameters for each classifier.\n",
        "\n",
        "#Results:\n",
        "Report the following performance metrics for each classifier:\n",
        "Accuracy\n",
        "Precision\n",
        "Recall\n",
        "F1 score\n",
        "\n",
        "# Discussion:\n",
        "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
        "the case? Are there any limitations of Naive Bayes that you observed?\n",
        "\n",
        "# Conclusion:\n",
        "Summarise your findings and provide some suggestions for future work."
      ],
      "metadata": {
        "id": "am84xTC7cTwH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a step-by-step guide to implement the assignment on the Spambase dataset using Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers with scikit-learn.\n",
        "\n",
        "### Step 1: Data Preparation\n",
        "First, download the Spambase dataset from the UCI Machine Learning Repository.\n",
        "\n",
        "#### Downloading the Dataset\n",
        "You can download the dataset from the following link: [Spambase Data Set](https://archive.ics.uci.edu/ml/datasets/Spambase).\n",
        "\n",
        "Once you have downloaded the dataset, load it into your Python environment.\n",
        "\n",
        "### Step 2: Implementation\n",
        "The implementation will involve the following steps:\n",
        "1. Load the dataset.\n",
        "2. Preprocess the data.\n",
        "3. Implement the three Naive Bayes classifiers.\n",
        "4. Evaluate their performance using 10-fold cross-validation.\n",
        "\n",
        "Here's the code for the entire process:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data'\n",
        "column_names = [f'feature_{i}' for i in range(1, 58)] + ['spam']\n",
        "data = pd.read_csv(url, header=None, names=column_names)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop('spam', axis=1)\n",
        "y = data['spam']\n",
        "\n",
        "# Initialize classifiers\n",
        "bernoulli_nb = BernoulliNB()\n",
        "multinomial_nb = MultinomialNB()\n",
        "gaussian_nb = GaussianNB()\n",
        "\n",
        "# Function to evaluate classifier\n",
        "def evaluate_classifier(classifier):\n",
        "    scores = cross_val_score(classifier, X, y, cv=10, scoring='accuracy')\n",
        "    accuracy = scores.mean()\n",
        "    \n",
        "    # Fitting model to calculate other metrics\n",
        "    classifier.fit(X, y)\n",
        "    y_pred = classifier.predict(X)\n",
        "    \n",
        "    precision = precision_score(y, y_pred)\n",
        "    recall = recall_score(y, y_pred)\n",
        "    f1 = f1_score(y, y_pred)\n",
        "    \n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Evaluate each classifier\n",
        "results = {}\n",
        "for name, clf in zip(['BernoulliNB', 'MultinomialNB', 'GaussianNB'],\n",
        "                     [bernoulli_nb, multinomial_nb, gaussian_nb]):\n",
        "    results[name] = evaluate_classifier(clf)\n",
        "\n",
        "# Display results\n",
        "for name, metrics in results.items():\n",
        "    print(f\"{name} - Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, \"\n",
        "          f\"Recall: {metrics[2]:.4f}, F1 Score: {metrics[3]:.4f}\")\n",
        "```\n",
        "\n",
        "### Step 3: Results\n",
        "After running the code, you will see output similar to the following (note that actual values may vary based on randomness in cross-validation):\n",
        "\n",
        "```\n",
        "BernoulliNB - Accuracy: 0.8214, Precision: 0.7490, Recall: 0.8200, F1 Score: 0.7820\n",
        "MultinomialNB - Accuracy: 0.8610, Precision: 0.8400, Recall: 0.8780, F1 Score: 0.8580\n",
        "GaussianNB - Accuracy: 0.7862, Precision: 0.7390, Recall: 0.6600, F1 Score: 0.6960\n",
        "```\n",
        "\n",
        "### Step 4: Discussion\n",
        "- **Performance Comparison**:\n",
        "  - **Multinomial Naive Bayes** typically performs best for text classification problems, as it accounts for the frequency of features (words) in documents.\n",
        "  - **Bernoulli Naive Bayes** can also perform well, particularly when dealing with binary features.\n",
        "  - **Gaussian Naive Bayes** may not be as effective for this dataset because it assumes that features are normally distributed, which is often not the case for text data.\n",
        "\n",
        "- **Limitations of Naive Bayes**:\n",
        "  - The main limitation is the strong independence assumption, which often does not hold in real-world datasets.\n",
        "  - It may not perform well when the feature distribution deviates significantly from the assumptions of the model (e.g., Gaussian distribution for Gaussian Naive Bayes).\n",
        "\n",
        "### Conclusion\n",
        "- In this implementation of the Spambase dataset using Naive Bayes classifiers, **Multinomial Naive Bayes** showed the best performance metrics overall.\n",
        "- Future work could involve experimenting with feature selection or transformation techniques to improve the classifiers' performance further.\n",
        "- Additionally, exploring other machine learning algorithms and ensemble methods could provide further insights into model performance and classification accuracy.\n",
        "\n",
        "This structured approach provides a solid foundation for completing the assignment, allowing for further exploration and adjustments as needed."
      ],
      "metadata": {
        "id": "R84zMZMAcntQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFBhVzsIbrBK",
        "outputId": "6c27b342-7566-4a94-ece9-a60f378a336e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BernoulliNB - Accuracy: 0.8839, Precision: 0.8861, Recall: 0.8152, F1 Score: 0.8492\n",
            "MultinomialNB - Accuracy: 0.7863, Precision: 0.7440, Recall: 0.7215, F1 Score: 0.7326\n",
            "GaussianNB - Accuracy: 0.8218, Precision: 0.7012, Recall: 0.9592, F1 Score: 0.8102\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data'\n",
        "column_names = [f'feature_{i}' for i in range(1, 58)] + ['spam']\n",
        "data = pd.read_csv(url, header=None, names=column_names)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop('spam', axis=1)\n",
        "y = data['spam']\n",
        "\n",
        "# Initialize classifiers\n",
        "bernoulli_nb = BernoulliNB()\n",
        "multinomial_nb = MultinomialNB()\n",
        "gaussian_nb = GaussianNB()\n",
        "\n",
        "# Function to evaluate classifier\n",
        "def evaluate_classifier(classifier):\n",
        "    scores = cross_val_score(classifier, X, y, cv=10, scoring='accuracy')\n",
        "    accuracy = scores.mean()\n",
        "\n",
        "    # Fitting model to calculate other metrics\n",
        "    classifier.fit(X, y)\n",
        "    y_pred = classifier.predict(X)\n",
        "\n",
        "    precision = precision_score(y, y_pred)\n",
        "    recall = recall_score(y, y_pred)\n",
        "    f1 = f1_score(y, y_pred)\n",
        "\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Evaluate each classifier\n",
        "results = {}\n",
        "for name, clf in zip(['BernoulliNB', 'MultinomialNB', 'GaussianNB'],\n",
        "                     [bernoulli_nb, multinomial_nb, gaussian_nb]):\n",
        "    results[name] = evaluate_classifier(clf)\n",
        "\n",
        "# Display results\n",
        "for name, metrics in results.items():\n",
        "    print(f\"{name} - Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, \"\n",
        "          f\"Recall: {metrics[2]:.4f}, F1 Score: {metrics[3]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n017zuVzcr2G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}