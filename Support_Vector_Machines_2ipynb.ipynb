{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?"
      ],
      "metadata": {
        "id": "BE-ktme3YCua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The relationship between polynomial functions and kernel functions in machine learning, particularly in Support Vector Machines (SVM) and other algorithms, is rooted in the ability of kernel functions to implicitly perform operations in higher-dimensional spaces without explicitly transforming the data. Here's an overview of this relationship:\n",
        "\n",
        "### 1. **Polynomial Functions**\n",
        "- A polynomial function is a mathematical expression involving a sum of powers in one or more variables, typically represented as:\n",
        "  \\[\n",
        "  f(x) = a_n x^n + a_{n-1} x^{n-1} + \\ldots + a_1 x + a_0\n",
        "  \\]\n",
        "- In the context of machine learning, polynomial functions can be used to model relationships between features, allowing for nonlinear decision boundaries.\n",
        "\n",
        "### 2. **Kernel Functions**\n",
        "- A kernel function is a method of computing the inner product of two vectors in a high-dimensional feature space without explicitly mapping the data points into that space. This is known as the **kernel trick**.\n",
        "- Common kernel functions include:\n",
        "  - Linear Kernel: \\( K(x, y) = x \\cdot y \\)\n",
        "  - Polynomial Kernel: \\( K(x, y) = (x \\cdot y + c)^d \\), where \\(c\\) is a constant and \\(d\\) is the degree of the polynomial.\n",
        "\n",
        "### 3. **Polynomial Kernel and Its Relation to Polynomial Functions**\n",
        "- The polynomial kernel can be seen as a direct extension of polynomial functions into a higher-dimensional space. When using a polynomial kernel in SVM, the decision boundary can be represented as a polynomial function of the input features.\n",
        "- The polynomial kernel allows the SVM to create complex decision boundaries by considering polynomial combinations of the input features. The kernel trick enables the SVM to find hyperplanes in this transformed feature space, making it possible to classify data that is not linearly separable in the original input space.\n",
        "\n",
        "### 4. **Advantages of Using Kernel Functions**\n",
        "- **Efficiency**: Kernel functions allow for efficient computation without needing to explicitly compute the coordinates in the high-dimensional space.\n",
        "- **Flexibility**: By choosing different kernel functions (linear, polynomial, radial basis function, etc.), algorithms can adapt to various data distributions and relationships, enabling them to model complex patterns.\n",
        "  \n",
        "### 5. **Examples of Applications**\n",
        "- In SVM, using a polynomial kernel allows the model to effectively classify data points that lie in a non-linear distribution, thereby achieving higher accuracy in certain cases.\n",
        "- Other algorithms, such as kernelized Principal Component Analysis (PCA), also utilize kernel functions to analyze data in higher-dimensional spaces.\n",
        "\n",
        "### Summary\n",
        "In summary, polynomial functions represent mathematical relationships in a feature space, while polynomial kernels provide a computationally efficient way to apply these relationships within machine learning algorithms by allowing implicit transformations to higher dimensions. This relationship enables the modeling of complex patterns and non-linear decision boundaries in various machine learning tasks."
      ],
      "metadata": {
        "id": "HE74eZHLYKuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
      ],
      "metadata": {
        "id": "leSJsqTVYLn4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "40BfCKuNX7gu",
        "outputId": "f254bcc1-a4fc-4eab-df03-461488187a3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[19  0  0]\n",
            " [ 0 12  1]\n",
            " [ 0  0 13]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       1.00      0.92      0.96        13\n",
            "           2       0.93      1.00      0.96        13\n",
            "\n",
            "    accuracy                           0.98        45\n",
            "   macro avg       0.98      0.97      0.97        45\n",
            "weighted avg       0.98      0.98      0.98        45\n",
            "\n",
            "Accuracy Score: 0.9777777777777777\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "X has 2 features, but SVC is expecting 4 features as input.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ab6977de7a9e>\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# Call the plot function using only the first two features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mplot_decision_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-ab6977de7a9e>\u001b[0m in \u001b[0;36mplot_decision_boundaries\u001b[0;34m(X, y, model)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Predict the class for each point in the mesh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    811\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    814\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \"\"\"\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m             X = self._validate_data(\n\u001b[0m\u001b[1;32m    607\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m                 \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    444\u001b[0m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                 \u001b[0;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: X has 2 features, but SVC is expecting 4 features as input."
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a SVM classifier with a polynomial kernel\n",
        "model = SVC(kernel='poly', degree=3, coef0=1, C=1.0)  # degree=3 for cubic polynomial\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Optional: Visualize decision boundaries (for first two features)\n",
        "def plot_decision_boundaries(X, y, model):\n",
        "    # Create a mesh grid\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
        "\n",
        "    # Predict the class for each point in the mesh\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot the decision boundary and the points\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', s=100)\n",
        "    plt.xlabel(iris.feature_names[0])  # Sepal length\n",
        "    plt.ylabel(iris.feature_names[1])  # Sepal width\n",
        "    plt.title('SVM Decision Boundary with Polynomial Kernel')\n",
        "    plt.show()\n",
        "\n",
        "# Call the plot function using only the first two features\n",
        "plot_decision_boundaries(X[:, :2], y, model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
      ],
      "metadata": {
        "id": "BkdLkCOLYSIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Support Vector Regression (SVR), the parameter epsilon (\\( \\epsilon \\)) plays a crucial role in defining the width of the epsilon-insensitive zone, which is the region around the predicted function within which errors are ignored. The relationship between epsilon and the number of support vectors can be understood as follows:\n",
        "\n",
        "### Impact of Increasing Epsilon on Support Vectors\n",
        "\n",
        "1. **Epsilon-Insensitive Zone**:\n",
        "   - The epsilon-insensitive zone is a region around the predicted values (the regression function) where errors (differences between predicted and actual values) are not penalized. This means that any predictions that fall within this zone do not contribute to the loss function.\n",
        "   - When \\( \\epsilon \\) is increased, the width of this zone increases, which means that a larger range of predicted values can be considered \"correct\" without incurring any penalty.\n",
        "\n",
        "2. **Effect on Support Vectors**:\n",
        "   - **Fewer Support Vectors**: As \\( \\epsilon \\) increases, more data points fall within the epsilon-insensitive zone, leading to fewer points being classified as support vectors. This is because only points that lie outside this zone contribute to the model's loss and are thus selected as support vectors.\n",
        "   - **Smoother Model**: A larger \\( \\epsilon \\) can lead to a smoother regression function, as it allows for more flexibility in ignoring minor deviations from the predicted values. This can help in avoiding overfitting, especially in noisy datasets.\n",
        "\n",
        "3. **Trade-offs**:\n",
        "   - **Bias-Variance Trade-off**: Increasing \\( \\epsilon \\) can reduce model complexity by decreasing the number of support vectors, which may lead to a simpler model with higher bias but lower variance. While this can prevent overfitting, it may also result in underfitting if \\( \\epsilon \\) is too large.\n",
        "   - **Model Performance**: The choice of \\( \\epsilon \\) should be made carefully, as it affects the model's ability to capture the underlying trends in the data. A balance needs to be struck to ensure that the model generalizes well to unseen data.\n",
        "\n",
        "### Summary\n",
        "In summary, increasing the value of epsilon in SVR typically leads to a reduction in the number of support vectors. This happens because a wider epsilon-insensitive zone allows more data points to be ignored in the loss calculation, resulting in fewer support vectors being needed to define the regression function. Adjusting \\( \\epsilon \\) is a crucial step in tuning the SVR model to achieve the desired trade-off between bias and variance."
      ],
      "metadata": {
        "id": "EE5b7JskYYwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?"
      ],
      "metadata": {
        "id": "vatbYp1BYZdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Support Vector Regression (SVR), several key parameters affect the model's performance. Understanding these parameters helps in tuning the model for better accuracy and generalization. Here's a breakdown of the kernel function, \\( C \\) parameter, \\( \\epsilon \\) parameter, and \\( \\gamma \\) parameter, along with their roles and effects on SVR performance:\n",
        "\n",
        "### 1. Kernel Function\n",
        "- **Function**: The kernel function determines how the input data is transformed into a higher-dimensional space. It allows SVR to capture non-linear relationships.\n",
        "- **Common Kernels**:\n",
        "  - **Linear Kernel**: Assumes a linear relationship. Best for linearly separable data.\n",
        "  - **Polynomial Kernel**: Captures polynomial relationships. Use when relationships in data are polynomial.\n",
        "  - **Radial Basis Function (RBF) Kernel**: Effective for non-linear relationships and generally the default choice. It can handle cases where the relationship is not easily defined.\n",
        "  \n",
        "- **Impact on Performance**:\n",
        "  - **Choice of Kernel**: Choosing the right kernel can significantly affect performance. For example, if the underlying relationship in the data is linear, using a linear kernel may yield better performance. Conversely, if the relationship is complex, an RBF or polynomial kernel may be more suitable.\n",
        "  - **Example**: For data with circular patterns, an RBF kernel might be more effective than a linear kernel.\n",
        "\n",
        "### 2. C Parameter\n",
        "- **Function**: The \\( C \\) parameter controls the trade-off between achieving a low training error and maintaining a low model complexity (regularization). A larger \\( C \\) aims to minimize the error on the training set.\n",
        "  \n",
        "- **Impact on Performance**:\n",
        "  - **High \\( C \\)**: The model focuses on minimizing training errors, leading to a more complex model. This may result in overfitting, where the model captures noise in the data rather than the underlying trend.\n",
        "  - **Low \\( C \\)**: The model allows for more margin violations, which can lead to underfitting but improves generalization on unseen data.\n",
        "\n",
        "- **Example**: If you notice that your model performs well on training data but poorly on validation data, consider decreasing \\( C \\) to reduce overfitting.\n",
        "\n",
        "### 3. Epsilon Parameter (\\( \\epsilon \\))\n",
        "- **Function**: The \\( \\epsilon \\) parameter defines the width of the epsilon-insensitive zone around the predicted values, where errors are not penalized.\n",
        "  \n",
        "- **Impact on Performance**:\n",
        "  - **High \\( \\epsilon \\)**: More data points are ignored, leading to fewer support vectors. This can simplify the model but might overlook important trends (underfitting).\n",
        "  - **Low \\( \\epsilon \\)**: More points are penalized, resulting in a model that closely follows the training data (risk of overfitting).\n",
        "\n",
        "- **Example**: If your model is too sensitive to noise in the training data, consider increasing \\( \\epsilon \\) to allow some margin for error.\n",
        "\n",
        "### 4. Gamma Parameter (\\( \\gamma \\))\n",
        "- **Function**: The \\( \\gamma \\) parameter determines the influence of a single training example. It controls the shape of the decision boundary in non-linear kernels (like RBF).\n",
        "  \n",
        "- **Impact on Performance**:\n",
        "  - **High \\( \\gamma \\)**: The decision boundary becomes very sensitive to individual data points, potentially leading to overfitting. The model can capture complex relationships but may generalize poorly.\n",
        "  - **Low \\( \\gamma \\)**: The influence of individual data points is more generalized, leading to a smoother decision boundary, which can help in capturing broader trends (risk of underfitting).\n",
        "\n",
        "- **Example**: If your model is too flexible and captures noise, decreasing \\( \\gamma \\) might improve generalization.\n",
        "\n",
        "### Summary\n",
        "- **Kernel Function**: Choose based on the data's underlying relationship.\n",
        "- **C Parameter**: Adjust to balance bias and variance (overfitting vs. underfitting).\n",
        "- **Epsilon Parameter**: Set to control the tolerance for errors around the predictions.\n",
        "- **Gamma Parameter**: Adjust to influence the complexity of the decision boundary.\n",
        "\n",
        "### Practical Tips\n",
        "1. **Hyperparameter Tuning**: Use techniques like Grid Search or Random Search with cross-validation to find the best combination of these parameters.\n",
        "2. **Evaluation**: Regularly evaluate model performance on validation datasets to ensure that adjustments are improving generalization rather than merely fitting the training data.\n",
        "\n",
        "By carefully tuning these parameters, you can significantly enhance the performance of your SVR model."
      ],
      "metadata": {
        "id": "cgAWYSltYg3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. Assignment:\n",
        "*  Import the necessary libraries and load the dataseg\n",
        "*  Split the dataset into training and testing setZ\n",
        "*  Preprocess the data using any technique of your choice (e.g. scaling, normalizationK\n",
        "* Create an instance of the SVC classifier and train it on the training datW\n",
        "*  Use the trained classifier to predict the labels of the testing datW\n",
        "*  Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
        "precision, recall, F1-scoreK\n",
        "*  Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV to\n",
        "improve its performanc_\n",
        "*  Train the tuned classifier on the entire dataseg\n",
        "* Save the trained classifier to a file for future use.\n",
        "  \n"
      ],
      "metadata": {
        "id": "mVb1IM-vYhyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import the necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Step 2: Load the dataset (Using Iris dataset as an example)\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 3: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Preprocess the data (Standard scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 5: Create an instance of the SVC classifier and train it\n",
        "svc = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Use the trained classifier to predict the labels of the testing data\n",
        "y_pred = svc.predict(X_test)\n",
        "\n",
        "# Step 7: Evaluate the performance of the classifier\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Step 8: Tune the hyperparameters using GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': ['scale', 'auto', 0.01, 0.1],\n",
        "    'kernel': ['rbf', 'linear', 'poly']\n",
        "}\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Step 9: Train the tuned classifier on the entire dataset\n",
        "best_svc = grid_search.best_estimator_\n",
        "best_svc.fit(X, y)\n",
        "\n",
        "# Step 10: Save the trained classifier to a file for future use\n",
        "joblib.dump(best_svc, 'trained_svc_model.joblib')\n",
        "\n",
        "print(\"Model saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsqvGzJIYRr4",
        "outputId": "e37892de-0d23-483a-8c68-bd599adf6b24"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       1.00      1.00      1.00        13\n",
            "           2       1.00      1.00      1.00        13\n",
            "\n",
            "    accuracy                           1.00        45\n",
            "   macro avg       1.00      1.00      1.00        45\n",
            "weighted avg       1.00      1.00      1.00        45\n",
            "\n",
            "Best Parameters: {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
            "Model saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CU5gy-6eY4hj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}