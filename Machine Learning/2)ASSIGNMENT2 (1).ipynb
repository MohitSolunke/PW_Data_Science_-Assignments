{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b38847e1-0eb3-4193-a657-94ad6a9c1c4f",
   "metadata": {},
   "source": [
    "# Assignment No.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8f7bb8-4841-4103-97a3-18c3ed3d9e5e",
   "metadata": {},
   "source": [
    "## Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how \n",
    "## can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bbe151-4bbb-490d-8f0b-7874c2ffbd54",
   "metadata": {},
   "source": [
    "Overfitting  :-  Overfitting is situation ,which is occurs when the Accuracy of Train Data is good and Test data is poor ,means there have a low bias and high variance . \n",
    "\n",
    "Underfitting :- Underfitting is situation ,which is occurs when the Accuracy of Train Data is poor and Test data is poor ,means there have a high bias and high variance . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c159b50-4d39-4955-9398-b1fd76f8907d",
   "metadata": {},
   "source": [
    " ##  Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a751b01-97df-4695-82e5-7947a637af6f",
   "metadata": {},
   "source": [
    "We can reduce the Overfitting by keeping the large traning data than the test data because if we pass or train the model with the large training data then it is might be less chances to give low accuracy on tested data because we are already train the models with all types of data\n",
    "\n",
    "Increase training data.\n",
    "Reduce model complexity.\n",
    "Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).\n",
    "Ridge Regularization and Lasso Regularization.\n",
    "Use dropout for neural networks to tackle overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcb565c-65e7-4411-8adf-051a1edaea92",
   "metadata": {},
   "source": [
    " ##  Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19deb975-4346-4450-9c9a-375796685280",
   "metadata": {},
   "source": [
    "Bias :- Bias is depend on the train dataset and it alternate of the accuracy of the model if the accuaracy of train data is higher then there is low bias and it affect on Model performance ,if there have high bias Model Performance is poor\n",
    "\n",
    "Variance :- Variance is depend on the test dataset and it alternate of the accuracy of the model if the accuaracy of test data is higher then there is low varianceand it affect on Model performance ,if there have high variance Model Performance is poor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bae93d-84bf-43b2-9021-f7c466ab6c1a",
   "metadata": {},
   "source": [
    " Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8215d055-9139-406d-bf89-4d4059350a14",
   "metadata": {},
   "source": [
    "A statistical model is said to be overfitted when the model does not make accurate predictions on testing data. When a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our data set. And when testing with test data results in High variance. Then the model does not categorize the data correctly, because of too many details and noise. The causes of overfitting are the non-parametric and non-linear methods because these types of machine learning algorithms have more freedom in building the model based on the dataset and therefore they can really build unrealistic models. A solution to avoid overfitting is using a linear algorithm if we have linear data or using the parameters like the maximal depth if we are using decision trees. \n",
    "\n",
    "In a nutshell, Overfitting is a problem where the evaluation of machine learning algorithms on training data is different from unseen data.\n",
    "\n",
    "Reasons for Overfitting:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad55a01-ce77-4323-b799-4ba48f116412",
   "metadata": {},
   "source": [
    ": Compare and contrast bias and variance in machine learning. What are some examples of high bias \n",
    "and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f4ddaa-beae-4097-8943-74dfccc8795a",
   "metadata": {},
   "source": [
    "Bias : -  1) When an algorithm is employed in a machine learning model and it does not fit well, a phenomenon known as bias can develop. Bias arises in several situations\n",
    " \n",
    "2)The disparity between the values that were predicted and the values that were actually observed is referred to as bias.\n",
    "\n",
    "3)The model is incapable of locating patterns in the dataset that it was trained on, and it produces inaccurate results for both seen and unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab6730a-c76c-458d-951b-0872b7254bea",
   "metadata": {},
   "source": [
    "Variance :- 1) The term \"variance\" refers to the degree of change that may be expected in the estimation of the target function as a result of using multiple sets of training data.\n",
    "\n",
    "2)A random variable's variance is a measure of how much it varies from the value that was predicted for it.\n",
    "\n",
    "3)The model recognizes the majority of the dataset's patterns and can even learn from the noise or data that isn't vital to its operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715879e3-0514-47b7-999b-097fba6d9640",
   "metadata": {},
   "source": [
    " Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe \n",
    "some common regularization techniques and how they work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2a8a45-d2d3-4892-a47c-bea7cc6e0e50",
   "metadata": {},
   "source": [
    "Regularization is a technique used to reduce errors by fitting the function appropriately on the given training set and avoiding overfitting. The commonly used regularization techniques are : \n",
    "\n",
    "1) Lasso Regularization – L1 Regularization\n",
    "2) Ridge Regularization – L2 Regularization\n",
    "3) Elastic Net Regularization – L1 and L2 Regularization\n",
    "\n",
    "1) Lasso Regression\n",
    "A regression model which uses the L1 Regularization technique is called LASSO(Least Absolute Shrinkage and Selection Operator) regression. Lasso Regression adds the “absolute value of magnitude” of the coefficient as a penalty term to the loss function(L). Lasso regression also helps us achieve feature selection by penalizing the weights to approximately equal to zero if that feature does not serve any purpose in the model.\n",
    "\n",
    "2)Ridge Regression\n",
    "A regression model that uses the L2 regularization technique is called Ridge regression. Ridge regression adds the “squared magnitude” of the coefficient as a penalty term to the loss function(L).\n",
    "\n",
    "3)Elastic Net Regression\n",
    "This model is a combination of L1 as well as L2 regularization. That implies that we add the absolute norm of the weights as well as the squared measure of the weights. With the help of an extra hyperparameter that controls the ratio of the L1 and L2 regularization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa10ff-9efa-4229-b0b6-f8d1691d0e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
